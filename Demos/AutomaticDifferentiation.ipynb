{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "AutomaticDifferentiation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbeckr/convex-optimization-class/blob/master/Demos/AutomaticDifferentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0cSUNfRGKN"
      },
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "Code resources\n",
        "#### Python\n",
        "Some of the big ones are:\n",
        "- Tensorflow\n",
        "- PyTorch\n",
        "- [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
        "All these Python packages do more than just AD\n",
        "\n",
        "#### Matlab\n",
        "- [ADiMat](https://www.sc.informatik.tu-darmstadt.de/res/sw/adimat/general/index.en.jsp) does reverse-mode AD\n",
        "There are not a lot of high-quality reverse-mode AD packages in Matlab\n",
        "\n",
        "#### Julia\n",
        "There are a lot of good choices, and it will only get better as more people use Julia\n",
        "- See [JuliaDiff](https://juliadiff.org/) for a curated list of packages, and the 2020 answer by ChrisRackauckas in this [forum post](https://discourse.julialang.org/t/state-of-automatic-differentiation-in-julia/43083) has a good discussion of pros and cons (e.g., which can do reverse mode, which can do Hessian-vector products, etc.)\n",
        "\n",
        "#### Fortran and classic scientific computing languagse\n",
        "- see [autodiff.org](http://www.autodiff.org/?module=Tools), e.g., [ADIFOR 2.0](https://www.mcs.anl.gov/research/projects/adifor/) for Fortran77\n",
        "\n",
        "\n",
        "## Demo\n",
        "This demo is forked from the version at [github.com/cu-numcomp/numcomp-class/blob/master/Differentiation.ipynb](https://github.com/cu-numcomp/numcomp-class/blob/master/Differentiation.ipynb) which was written by Prof. Jed Brown for CU's CSCI 3656 \"Numerical Computation\" Spring 2020, released under the simplified BSD license. (The original version has demos on finite differences as well, which are not in this version)\n",
        "\n",
        "Copyright (c) 2017-2020, Jed Brown\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without modification,\n",
        "are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "* Redistributions in binary form must reproduce the above copyright notice, this\n",
        "  list of conditions and the following disclaimer in the documentation and/or\n",
        "  other materials provided with the distribution.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
        "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWYWp5OpQ_cB"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHTln3qSQ_cM"
      },
      "source": [
        "# Symbolic differentiation\n",
        "\n",
        "We've been differentiating basic mathematical functions, for which there is a formula for the derivative.\n",
        "Symbolic differentiation is a tool that can compute those expressions (and generate code to evaluate the expressions numerically)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxQFyyRQ_cN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "276a1227-5d70-4fd5-8db4-909c38723ecf"
      },
      "source": [
        "import sympy\n",
        "from sympy.abc import x\n",
        "\n",
        "f = sympy.cos(x**sympy.pi) * sympy.log(x)\n",
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}$",
            "text/plain": [
              "log(x)*cos(x**pi)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3E2DwL_Q_cN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3148b7d8-16e2-455e-adbb-ddba68d19c0a"
      },
      "source": [
        "sympy.diff(f, x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle - \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}$",
            "text/plain": [
              "-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtlGXPzQQ_cN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ed235fd-7633-4bcd-eb8c-25798b21c2fc"
      },
      "source": [
        "sympy.ccode(f, 'y')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'y = log(x)*cos(pow(x, M_PI));'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5uQ14BmQ_cN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25d7b2e0-ff80-4023-815c-69790fd2fe2d"
      },
      "source": [
        "sympy.fcode(f, 'y')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'      y = log(x)*cos(x**3.1415926535897932d0)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sz1_sIkQ_cO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "2fef77cc-5f8b-4b05-cdda-7d8e79aea6e3"
      },
      "source": [
        "f.evalf(40, subs={x: 1.9})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle 0.215513413838041906745231945917755720873$",
            "text/plain": [
              "0.2155134138380419067452319459177557208730"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA_KQA-WULoM"
      },
      "source": [
        "#### A more complicated function\n",
        "and its derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu_iQKxsQ_cO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "780aaf04-e33e-483e-a5fd-8ff5718485e7"
      },
      "source": [
        "def g(x, m=np):\n",
        "    y = x\n",
        "    for i in range(2):\n",
        "        # a = m.log(y)\n",
        "        # b = y ** m.pi\n",
        "        # c = m.cos(b)\n",
        "        # y = c * a\n",
        "        y = m.cos(y**m.pi) * m.log(y)\n",
        "    return y\n",
        "\n",
        "gexpr = g(x, m=sympy)\n",
        "gexpr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}$",
            "text/plain": [
              "log(log(x)*cos(x**pi))*cos((log(x)*cos(x**pi))**pi)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWQNoqi4Q_cO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "59dcae0f-dde0-4503-f720-2238ca20d1c8"
      },
      "source": [
        "sympy.diff(gexpr, x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle - \\frac{\\pi \\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\sin{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}} + \\frac{\\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}}$",
            "text/plain": [
              "-pi*(log(x)*cos(x**pi))**pi*(-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*log(log(x)*cos(x**pi))*sin((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi)) + (-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*cos((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s3P7vo-UPvI"
      },
      "source": [
        "#### Another complicated function\n",
        "An example of a Speelpenning function (from [Peder Olsen's slides](https://researcher.watson.ibm.com/researcher/files/us-pederao/ADTalk.pdf) )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9nrggL0Uahm"
      },
      "source": [
        "m = 8\n",
        "t = np.random.randn(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esqzYI7ZXZx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "2f16f863-9dd0-4829-9f89-87275c50622d"
      },
      "source": [
        "def f(x):\n",
        "  y = 1\n",
        "  for i in range(m):\n",
        "    y *= x - t[i]\n",
        "  return y\n",
        "\n",
        "f(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right)$",
            "text/plain": [
              "(x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwM9u67EYC2E"
      },
      "source": [
        "But $f$ is just a polynomial!  Is `sympy` clever enough to do the right thing?  No."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUMn0kfdXyFb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "7801fa99-6411-4a2c-e843-032643cb2821"
      },
      "source": [
        "sympy.diff(f(x), x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 1.64336789993968\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.8588931303315\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right) + \\left(x - 1.64336789993968\\right) \\left(x - 0.80831376120146\\right) \\left(x - 0.61716521569881\\right) \\left(x - 0.21872464815123\\right) \\left(x + 0.283354811393787\\right) \\left(x + 0.484169738605473\\right) \\left(x + 1.22578415282071\\right)$",
            "text/plain": [
              "(x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473) + (x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 1.22578415282071) + (x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.484169738605473)*(x + 1.22578415282071) + (x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071) + (x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071) + (x - 1.8588931303315)*(x - 1.64336789993968)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071) + (x - 1.8588931303315)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071) + (x - 1.64336789993968)*(x - 0.80831376120146)*(x - 0.61716521569881)*(x - 0.21872464815123)*(x + 0.283354811393787)*(x + 0.484169738605473)*(x + 1.22578415282071)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awwPmZV7YetF"
      },
      "source": [
        "`sympy` does the right thing if we give it some help..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydK2lEv2YRDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "3fae0386-93ba-4424-a120-276151477c4c"
      },
      "source": [
        "sympy.expand( f(x) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle x^{8} - 3.15315595250272 x^{7} + 0.443444575152368 x^{6} + 5.83266528338558 x^{5} - 3.52089712670291 x^{4} - 1.60974734454948 x^{3} + 1.07667017812041 x^{2} + 0.121387988271663 x - 0.0560545147667825$",
            "text/plain": [
              "x**8 - 3.15315595250272*x**7 + 0.443444575152368*x**6 + 5.83266528338558*x**5 - 3.52089712670291*x**4 - 1.60974734454948*x**3 + 1.07667017812041*x**2 + 0.121387988271663*x - 0.0560545147667825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92wa0dtEYZn4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "4c6d2c77-d9df-4074-9322-41154618e327"
      },
      "source": [
        "sympy.diff(sympy.expand( f(x) ), x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle 8 x^{7} - 22.072091667519 x^{6} + 2.66066745091421 x^{5} + 29.1633264169279 x^{4} - 14.0835885068116 x^{3} - 4.82924203364843 x^{2} + 2.15334035624082 x + 0.121387988271663$",
            "text/plain": [
              "8*x**7 - 22.072091667519*x**6 + 2.66066745091421*x**5 + 29.1633264169279*x**4 - 14.0835885068116*x**3 - 4.82924203364843*x**2 + 2.15334035624082*x + 0.121387988271663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0onNB1MFQ_cO"
      },
      "source": [
        "# Hand-coding derivatives\n",
        "\n",
        "The size of these expressions grow exponentially in the number of loop iterations, yet one can write efficient code for computing the derivative by hand.  We use the variational notation\n",
        "\n",
        "$$ \\operatorname{d} f = f'(x) \\operatorname{d} x $$\n",
        "\n",
        "which allows us to break a large computation into simple pieces that we can compute incrementally, instead of trying to build up expressions for complicated functions.  That is, we can differentiate a composition $h(g(f(x)))$ as\n",
        "\n",
        "\\begin{align}\n",
        "  \\operatorname{d} h &= h' \\operatorname{d} g \\\\\n",
        "  \\operatorname{d} g &= g' \\operatorname{d} f \\\\\n",
        "  \\operatorname{d} f &= f' \\operatorname{d} x.\n",
        "\\end{align}\n",
        "Consider our example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwUEJThFQ_cP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93ebd81-0bb2-43e5-a3a8-c2c845f0161d"
      },
      "source": [
        "def gprime(x):\n",
        "    y = x\n",
        "    dy = 1\n",
        "    for i in range(2):\n",
        "        a = np.log(y)\n",
        "        da = 1/y * dy\n",
        "        b = y ** np.pi\n",
        "        db = np.pi * y ** (np.pi - 1) * dy\n",
        "        c = np.cos(b)\n",
        "        dc = -np.sin(b) * db\n",
        "        y = c * a\n",
        "        dy = dc * a + c * da\n",
        "    return y, dy\n",
        "\n",
        "print('by hand', gprime(1.9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "by hand (-1.5346823414986814, -34.03241959914048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1ydNnccQ_cP"
      },
      "source": [
        "* This code is pretty mechanical to write\n",
        "* It's hard to maintain as you add new features\n",
        "* It's hard to debug\n",
        "  * You can test using finite differencing\n",
        "  * You can take apart pieces for unit testing and/or debugging\n",
        "* If you know you'll be writing this sort of code, plan ahead!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Gx8g62Q_cP"
      },
      "source": [
        "### Variational notation is handy (an example)\n",
        "\n",
        "We'll differentiate the expression\n",
        "\n",
        "$$ I = A^{-1} A $$\n",
        "applying the product rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e29pE7i9Q_cP"
      },
      "source": [
        "$$ 0 = A^{-1} (\\operatorname dA) + (\\operatorname dA^{-1}) A $$\n",
        "and collect terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HcKqcMZQ_cP"
      },
      "source": [
        "$$ \\operatorname dA^{-1} = - A^{-1} (\\operatorname dA) A^{-1}. $$\n",
        "\n",
        "This expression for the derivative $\\operatorname d A^{-1}$ in direction $\\operatorname d A$ is useful when differentiating algorithmn that involve linear algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ouWZwoFQ_cP"
      },
      "source": [
        "## Reverse-mode\n",
        "\n",
        "What we've done above is called \"forward mode\", and amounts to placing the parentheses in the chain rule like\n",
        "\n",
        "$$ \\operatorname d h = \\frac{dh}{dg} \\left(\\frac{dg}{df} \\left(\\frac{df}{dx} \\operatorname d x \\right) \\right) .$$\n",
        "\n",
        "The expression means the same thing if we rearrange the parentheses,\n",
        "\n",
        "$$ \\operatorname d h = \\left( \\left( \\left( \\frac{dh}{dg} \\right) \\frac{dg}{df} \\right) \\frac{df}{dx} \\right) \\operatorname d x .$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yy6W0TQQ_cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e369e17a-8701-4437-ac4c-2ba87142ba96"
      },
      "source": [
        "def gprime_rev(x):\n",
        "    # First compute all the values by going through the iteration forwards\n",
        "    # I'm unrolling two iterations here for clarity (\"static single assignment\" form)\n",
        "    # It is possible to write code that keeps the loop structure.\n",
        "    a1 = np.log(x)\n",
        "    b1 = x ** np.pi\n",
        "    c1 = np.cos(b1)\n",
        "    y1 = c1 * a1\n",
        "    a2 = np.log(y1)\n",
        "    b2 = y1 ** np.pi\n",
        "    c2 = np.cos(b2)\n",
        "    y = c2 * a2 # Result\n",
        "    # Now go backwards computing dy/d_ for each variable\n",
        "    y_ = 1\n",
        "    y_c2 = y_ * a2\n",
        "    y_a2 = c2 * y_\n",
        "    y_b2 = -y_c2 * np.sin(b2) # dy/db2 = dy/dc2 dc2/db2\n",
        "    y_y1 = y_b2 * np.pi * y1 ** (np.pi - 1) + y_a2 / y1\n",
        "    y_c1 = y_y1 * a1\n",
        "    y_a1 = c1 * y_y1\n",
        "    y_b1 = -y_c1 * np.sin(b1)\n",
        "    y_x = y_b1 * np.pi * x ** (np.pi - 1) + y_a1 / x\n",
        "    return y, y_x\n",
        "\n",
        "print('forward', gprime(1.9))\n",
        "print('reverse', gprime_rev(1.9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forward (-1.5346823414986814, -34.03241959914048)\n",
            "reverse (-1.5346823414986814, -34.03241959914049)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r7GIOErQ_cQ"
      },
      "source": [
        "* This is fairly mechanical, similar to forward-mode\n",
        "* It is more complicated than forward-mode\n",
        "* This sort of code is tricky to debug\n",
        "  * You can test using forward-mode or finite differencing\n",
        "* We need the results of intermediate computation in reverse order\n",
        "  * We have to store those values somewhere (\"taping\" in the literature)\n",
        "  * Or we have to recompute them (see \"hierarchical checkpointing\")\n",
        "* Reverse-mode is also known as the \"adjoint\" method and \"back-propagation\".\n",
        "  \n",
        "### Why reverse?\n",
        "\n",
        "If all we had was scalar functions of scalar inputs, we would never use reverse mode.  But let's suppose we are given a dot product with a constant vector.\n",
        "\n",
        "$$ f(\\mathbf x) = \\mathbf c^T \\mathbf x = \\begin{pmatrix} c_0 & c_1 & c_2 & \\dotsb \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\end{pmatrix} $$\n",
        "and wish to compute the gradient\n",
        "$$ \\nabla_{\\mathbf x} f = \\frac{\\partial f}{\\partial \\mathbf x} = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_0} & \\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\dotsb \\end{pmatrix} . $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2JudAgnQ_cQ",
        "outputId": "e78d4f4d-8974-4783-d8f6-661c6cc7ad2f"
      },
      "source": [
        "def dot(c, x):\n",
        "    n = len(c)\n",
        "    sum = 0\n",
        "    for i in range(n):\n",
        "        sum += c[i] * x[i]\n",
        "    return sum\n",
        "        \n",
        "n = 20\n",
        "c = np.random.randn(n)\n",
        "x = np.random.randn(n)\n",
        "f = dot(c, x)\n",
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7881902865938712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-znaZxA2Q_cQ"
      },
      "source": [
        "If we use forward mode, we can only compute one direction at a time, effectively\n",
        "$$ \\left(\\nabla_{\\mathbf x} f\\right) \\cdot \\operatorname d x $$\n",
        "for one value of the vector $\\operatorname d x$ at a time.\n",
        "We can compute the full gradient by choosing $\\operatorname d x$ to be each column of the identity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oruLzUmNQ_cQ",
        "outputId": "a7d2910c-4ff5-4b9a-88df-45b72dfe6060"
      },
      "source": [
        "def dot_x(c, x, dx):\n",
        "    \"\"\"Compute derivative in direction dx\"\"\"\n",
        "    n = len(c)\n",
        "    dsum = 0\n",
        "    for i in range(n):\n",
        "        dsum += c[i] * dx[i]\n",
        "    return dsum\n",
        "\n",
        "def grad_dot(c, x):\n",
        "    n = len(c)\n",
        "    I = np.eye(n)\n",
        "    grad = np.zeros(n)\n",
        "    for j in range(n):\n",
        "        dx = I[:,j]\n",
        "        grad[j] = dot_x(c, x, dx)\n",
        "    return grad\n",
        "\n",
        "grad_dot(c, x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.41288309, -0.02618   , -1.63423663,  0.3006944 , -0.25078029,\n",
              "       -0.45382014,  1.94208405,  0.30730437,  0.45064477, -0.89218614,\n",
              "       -1.25475455, -0.76033123, -0.54544687, -1.40029722, -0.74489052,\n",
              "       -0.18788402, -1.00248867,  1.5845576 , -0.29049147,  0.98277278])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEaKrJqVQ_cR"
      },
      "source": [
        "We've now traversed the loop with our work as many times as there are components in the vector.  The forward evaluation for `dot` costs $O(n)$ and computing the gradient costs $O(n^2)$ because we have to do $O(n)$ for for each direction and there are $n$ directions.\n",
        "\n",
        "Compare with reverse-mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nea1QjtpQ_cR",
        "outputId": "72db173c-43f8-4143-cff3-ffbecb7bd747"
      },
      "source": [
        "def grad_dot_rev(c, x):\n",
        "    n = len(c)\n",
        "    sum_ = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        sum_[i] = c[i]\n",
        "    return sum_\n",
        "\n",
        "grad_dot_rev(c, x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.41288309, -0.02618   , -1.63423663,  0.3006944 , -0.25078029,\n",
              "       -0.45382014,  1.94208405,  0.30730437,  0.45064477, -0.89218614,\n",
              "       -1.25475455, -0.76033123, -0.54544687, -1.40029722, -0.74489052,\n",
              "       -0.18788402, -1.00248867,  1.5845576 , -0.29049147,  0.98277278])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A36BmCPCQ_cR"
      },
      "source": [
        "* We get the same values in only $O(n)$ work!\n",
        "* The astute reader may recall that we already worked out this case,\n",
        "$$ \\frac{\\partial \\mathbf c^T \\mathbf x}{\\partial \\mathbf x} = \\mathbf c^T .$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52EwIxuQ_cR"
      },
      "source": [
        "## Shape of the gradient (Jacobian)\n",
        "\n",
        "Suppose we have a vector-valued function of vector-valued input, $\\mathbf f(\\mathbf x)$ where $\\mathbf f$ has length $m$ and $\\mathbf x$ has length $n$.\n",
        "* The gradient (Jacobian) matrix $J = \\nabla_{\\mathbf x} \\mathbf f$ has shape $m\\times n$.\n",
        "* Usually in optimization, $m=1$ because we only have one objective\n",
        "* If $m\\ll n$ then finite differencing and forward-mode differentiation will be much more expensive than reverse-mode differentiation\n",
        "  * Find a way to use reverse-mode!\n",
        "* If $m \\approx n$ then either is about as efficient, but forward-mode is simpler.\n",
        "* If $m \\gg n$ then forward-mode is the ticket.\n",
        "* In real computations, there may be expensive stages that have lower dimension inputs or outputs, in which case those can be captured. An example is\n",
        "$$ \\mathbf f(\\mathbf x) = \\mathbf q \\sigma(\\mathbf q^T \\mathbf x) $$\n",
        "where $\\sigma$ is an expensive nonlinear function.\n",
        "The Jacobian $J = \\nabla_{\\mathbf x} \\mathbf f$ is a square matrix, but naive forward- and reverse-mode would both require $n$ evaluations of $\\sigma$.\n",
        "Since $\\sigma$ is a scalar-valued function of a scalar argument, $\\sigma'(\\mathbf q^T \\mathbf x)$ is just one number, and thus $J = (\\sigma') \\mathbf q \\mathbf q^T$ is readily available (and you know it's rank-1 so don't need to store all $n^2$ entries). Models of this sort show up frequently in physical modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnVmNxnQ_cR"
      },
      "source": [
        "# Algorithmic (automatic) differentiation\n",
        "\n",
        "Next, we'll consider ways to have libraries/compilers generate by-hand code such as we see above.\n",
        "We'll use the [JAX](https://jax.readthedocs.io/en/latest/) library, which offers differentiation of NumPy computations (and offload to GPUs, which we won't use now).\n",
        "Uncomment the line below if you need to install `jax` and `jaxlib`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOAKnLr5Q_cS"
      },
      "source": [
        "# ! pip install jax jaxlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqTsEWoTQ_cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11cade1-6177-42e9-a154-b548d94ba444"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# def g_jax(x):\n",
        "#     \"\"\"Same function as before, but using jnp in place of np.\"\"\"\n",
        "#     y = x\n",
        "#     for i in range(2):\n",
        "#         y = jnp.cos(y**jnp.pi) * jnp.log(y)\n",
        "#     return y\n",
        "\n",
        "g_jax = lambda x : g(x, m=jnp)\n",
        "\n",
        "gprime_jax = jax.grad(g_jax)\n",
        "print(gprime_jax(1.9))\n",
        "print(gprime(1.9)[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-34.03244\n",
            "-34.03241959914048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VxasSM8WrMM"
      },
      "source": [
        "#### Example with linear function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywi_jF0ZQ_cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e3c909-932f-4dce-c043-c44317eea5c0"
      },
      "source": [
        "n = 20\n",
        "c = np.random.randn(n)\n",
        "y = np.random.randn(n)\n",
        "\n",
        "# SRB changing example a bit\n",
        "# Let h(y) = dot(y,c), then grad h = c\n",
        "h = lambda y : jnp.vdot(y,c)\n",
        "\n",
        "print(\"Gradient via JAX AD is\")\n",
        "print( jax.grad(h)(y) )\n",
        "# Alternatively,\n",
        "# jax.grad(jmp.vdot)(y,c) will pass in both y and c to the function\n",
        "#     but only differentiation with respect to the first input (in this case, y)\n",
        "\n",
        "print(\"Gradient worked out by hand is\")\n",
        "print( c )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient via JAX AD is\n",
            "[-0.7441569  -0.2227114  -0.15074983 -0.34559867 -0.3003243  -0.79840875\n",
            " -1.359969   -0.92792886 -2.007117    0.9300492  -1.0390164  -1.9017624\n",
            " -0.3267806   0.10536153  0.81950676  0.16529033  0.07327905 -0.46351126\n",
            " -1.5353583   0.20672266]\n",
            "Gradient worked out by hand is\n",
            "[-0.74415687 -0.2227114  -0.15074983 -0.34559866 -0.30032428 -0.79840873\n",
            " -1.35996901 -0.92792889 -2.00711701  0.93004917 -1.0390164  -1.90176237\n",
            " -0.32678058  0.10536153  0.81950676  0.16529033  0.07327905 -0.46351126\n",
            " -1.53535827  0.20672265]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzIXp43Q_cS",
        "outputId": "789485b8-ce40-4a59-90d2-5e344f4b5aae"
      },
      "source": [
        "jax.grad(dot, argnums=1)(c, x) - c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "             0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0sANssFQ_cS"
      },
      "source": [
        "## Software\n",
        "\n",
        "* Algorithmic differentiation (AD) software has been around for over 40 years\n",
        "* There are two classical approaches\n",
        "  * Source transformation: AD tool emits Fortran (or C, etc.) code, which is compiled by a normal compiler\n",
        "  * Operator overleading: each basic operation is overloaded to transform objects holding values + derivatives\n",
        "* Source transformation is usually more efficient, retaining loop structure, etc.\n",
        "* Implementations tend to have poor ergonomics, odd restrictions on use, poor composition.\n",
        "* Vectorization has been poor with most classical tools.\n",
        "* AD *implementations* have come a long way in the past few years (despite the math being old)\n",
        "* Just-in-time compilation and extensive software engineering\n",
        "* Exemplars:\n",
        "  * [JAX](https://jax.readthedocs.io/en/latest/) for Python\n",
        "  * [Zygote.jl](http://fluxml.ai/Zygote.jl/latest/) for Julia\n",
        "* AD is great within its domain, but is still intrusive (especially for multi-language projects, languages with poor AD tooling, etc.).  Even in JAX, you'll see [various constraints](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), such as that you can't in-place update an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaO9aGVMQ_cT",
        "outputId": "083598d8-a438-4ab8-96d4-60f6c889b1f1"
      },
      "source": [
        "z = jnp.zeros(3)\n",
        "z[1] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'<class 'jax.interpreters.xla.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2a7f31191bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   3394\u001b[0m          \u001b[0;34m\"immutable; perhaps you want jax.ops.index_update or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m          \"jax.ops.index_add instead?\")\n\u001b[0;32m-> 3396\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3398\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_operator_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<class 'jax.interpreters.xla.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MHMx5rpQ_cT"
      },
      "source": [
        "* If you work in this space, you'll eventually learn to judge when to use AD and when to hand-code a derivative.  This type of decision lies at the intersection of numerical analysis and software engineering."
      ]
    }
  ]
}