{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: RPCA\n",
    "\n",
    "In-class challenge: how many different algorithms could you use to solve the RPCA problem?\n",
    "\n",
    "APPM 5630 Adv Convex Optimization, Spring 2023, Professor Stephen Becker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import default_rng\n",
    "import cvxpy as cvx\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!wget -nv 'https://github.com/stephenbeckr/convex-optimization-class/raw/master/utilities/firstOrderMethods.py'\n",
    "!wget -nv 'https://github.com/stephenbeckr/convex-optimization-class/raw/master/utilities/secondOrderMethods.py'\n",
    "from firstOrderMethods import gradientDescent, lassoSolver, createTestProblem, runAllTestProblems, DouglasRachford, bookkeeper\n",
    "from secondOrderMethods import NewtonsMethod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Robust PCA (RPCA) problem\n",
    "$$\\min_{L,S} \\;\\frac12\\|L+S-Y\\|_F^2 + \\lambda_L\\|L\\|_* + \\lambda_S\\|S\\|_1$$\n",
    "where $L,S$ are matrices of the same size; $Y$ is the target matrix that we wish to approximately decompose, $Y \\approx L + S$, into a low-rank part $L$ and a sparse part $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(1)\n",
    "m,n = 7,8\n",
    "\n",
    "# for use later\n",
    "def vec(L,S):\n",
    "    return np.concatenate( (L.ravel(), S.ravel() ))\n",
    "def mat( LS ):\n",
    "    L = LS[:m*n].reshape( (m,n ))\n",
    "    S = LS[m*n:].reshape( (m,n ))\n",
    "    return L, S\n",
    "\n",
    "# Generate a \"signal\". Need not be solution to optimization problem\n",
    "rr  = 4\n",
    "LL  = rng.standard_normal( (m,rr) ) @ rng.standard_normal( (rr,n) ) # low-rank\n",
    "SS  = rng.standard_normal( (m,n) )\n",
    "SS[ np.abs(SS) < np.quantile( np.abs(SS), .9 ) ] = 0  # sparse\n",
    "Y   = LL + SS + .01*rng.standard_normal( (m,n) ) # Noisy observations\n",
    "\n",
    "# with np.printoptions(precision=3, suppress=True):\n",
    "#     print(SS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve in CVXPY for reference solution\n",
    "\n",
    "This is one way to solve: the $\\ell_1$ and nuclear norms can be written in terms of epigraphs of standard cones and such, and then solved via standard methods (IPM, ADMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.374  4.849  2.435  1.788  0.466  0.314  0.   ]\n",
      "[10.373  4.687  2.211  1.507  0.     0.     0.   ]\n",
      "[[ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-1.649  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -1.482  0.     0.     0.     0.    -1.631  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -2.251  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.    -1.514]\n",
      " [ 1.753  0.     0.     0.     0.     0.     0.     0.   ]]\n",
      "[[-0.    -0.    -0.     0.    -0.     0.     0.    -0.   ]\n",
      " [-0.54   0.     0.     0.     0.     0.    -0.     0.   ]\n",
      " [ 0.    -0.61  -0.     0.    -0.     0.    -0.666 -0.   ]\n",
      " [-0.    -0.    -0.    -0.     0.    -0.671 -0.    -0.   ]\n",
      " [ 0.     0.    -0.    -1.694 -0.     0.    -0.     0.   ]\n",
      " [ 0.     0.    -0.    -0.     0.     0.    -0.    -0.912]\n",
      " [ 0.     0.    -0.188  0.     0.    -0.    -0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Solve in cvxpy\n",
    "L     = cvx.Variable( (m,n) )\n",
    "S     = cvx.Variable( (m,n) )\n",
    "lambdaL = 3e-3\n",
    "lambdaS = 1.7e-3\n",
    "\n",
    "def prox_l1( X, t, lambdaS ):\n",
    "    return np.sign(X)*np.maximum( 0, np.fabs(X) - lambdaS*t )\n",
    "\n",
    "def prox_nuclearNorm( X, t, lambdaL ):\n",
    "   U, s, Vh = linalg.svd(X, full_matrices=False )\n",
    "   s = prox_l1( s, t, lambdaL )\n",
    "   return U@( s.reshape((-1,1))*Vh )\n",
    "\n",
    "def objective_vec( LSvec ):\n",
    "    L,S = mat(LSvec)\n",
    "    resid = norm( L+S - Y)\n",
    "    return resid**2/2 + lambdaL*np.sum( linalg.svdvals(L) ) + lambdaS*norm( S.ravel(), ord=1)\n",
    "obj   = cvx.Minimize( cvx.sum_squares(L+S-Y)/2 + lambdaL*cvx.norm(L, \"nuc\") + lambdaS*cvx.pnorm(S,p=1) )\n",
    "prob  = cvx.Problem(obj)\n",
    "highPrecision = {'solver':cvx.CVXOPT,'max_iters':400,'abstol':1e-11,'reltol':1e-11}\n",
    "prob.solve(**highPrecision)\n",
    "L_CVX = L.value \n",
    "S_CVX = S.value \n",
    "fTrue = prob.value\n",
    "\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print( linalg.svdvals( L_CVX ))\n",
    "    print( linalg.svdvals( LL ) )\n",
    "    print(SS)\n",
    "    print(S_CVX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal gradient descent (or FISTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06968472982246328\n",
      "Iter.  Objective Stepsize  Error\n",
      "-----  --------- --------  -------\n",
      "    0  9.38e-02  5.00e-01  6.79e-01\n",
      "  500  6.97e-02  5.00e-01  2.04e-02\n",
      " 1000  6.97e-02  5.00e-01  1.96e-03\n",
      " 1500  6.97e-02  5.00e-01  2.45e-04\n",
      " 2000  6.97e-02  5.00e-01  3.11e-05\n",
      " 2500  6.97e-02  5.00e-01  3.94e-06\n",
      " 2873  6.97e-02  5.00e-01  3.35e-07\n",
      "Iter 2873 Quitting due to stagnating objective value\n"
     ]
    }
   ],
   "source": [
    "def prox( LSvec, t ):\n",
    "    L,S = mat(LSvec)\n",
    "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
    "    S   = prox_l1( S, t, lambdaS)\n",
    "    return vec( L, S )\n",
    "def prox_obj( LSvec ):\n",
    "    L,S = mat(LSvec)\n",
    "    return lambdaL*np.sum( linalg.svdvals(L) ) + lambdaS*norm( S.ravel(), ord=1)\n",
    "\n",
    "def f( LSvec ):\n",
    "    L,S = mat(LSvec)\n",
    "    resid = norm( L+S - Y)\n",
    "    return resid**2/2\n",
    "\n",
    "def grad( LSvec ):\n",
    "    L,S = mat(LSvec)\n",
    "    res = L+S-Y\n",
    "    return np.concatenate( ( res.ravel(), res.ravel() ) )\n",
    "    \n",
    "print( fTrue )\n",
    "refSoln = vec( L_CVX, S_CVX)\n",
    "errFcn  = lambda LS : norm( LS-refSoln )/norm(refSoln)\n",
    "LSvec0 = np.zeros(2*m*n)  # starting point\n",
    "LS, data = gradientDescent( f, grad, LSvec0, prox, prox_obj, stepsize = 0.5, tol=1e-16, maxIters=1e4,\n",
    "    errorFunction=errFcn, ArmijoLinesearch=False, acceleration=True, restart=500, saveHistory=True )\n",
    "L,S = mat(LS)\n",
    "# with np.printoptions(precision=3, suppress=True):\n",
    "#     print( L )\n",
    "#     print( S )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Projection  (VarPro) + proximal gradient descent\n",
    "$$\\min_{L} \\lambda_L\\|L\\|_* + \\underbrace{\\min_S \\frac12\\|L+S-Y\\|_F^2+\\lambda_S\\|S\\|_1}_{f(L)}$$\n",
    "where\n",
    "$$\\nabla f(L) = L+S-Y\\quad\\text{where }S\\text{ solves the partial minimization problem}\n",
    "$$\n",
    "\n",
    "and of course you could also switch this around and flip the roles of $S$ and $L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06968472982246328\n",
      "Iter.  Objective Stepsize  Error\n",
      "-----  --------- --------  -------\n",
      "    0  1.15e-01  5.00e-01  1.39e+00\n",
      "  500  6.98e-02  5.00e-01  5.58e-02\n",
      " 1000  6.97e-02  5.00e-01  7.36e-04\n",
      " 1500  6.97e-02  5.00e-01  8.33e-06\n",
      " 1757  6.97e-02  5.00e-01  6.29e-07\n",
      "Iter 1757 Quitting due to stagnating objective value\n"
     ]
    }
   ],
   "source": [
    "def S_from_L( L ):\n",
    "    return prox_l1( Y-L, 1, lambdaS )\n",
    "\n",
    "def prox( Lvec, t ):\n",
    "    L   = Lvec.reshape( (m,n) )\n",
    "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
    "    return L.ravel()\n",
    "    \n",
    "def prox_obj( Lvec ):\n",
    "    L   = Lvec.reshape( (m,n) )\n",
    "    return lambdaL*np.sum( linalg.svdvals(L) )\n",
    "\n",
    "def f( Lvec ):\n",
    "    L   = Lvec.reshape( (m,n) )\n",
    "    S   = S_from_L(L)\n",
    "    resid = norm( L+S - Y)\n",
    "    return resid**2/2 + lambdaS*norm(S.ravel(),ord=1)\n",
    "\n",
    "def grad( Lvec ):\n",
    "    L   = Lvec.reshape( (m,n) )\n",
    "    S   = S_from_L(L)\n",
    "    res = L+S-Y\n",
    "    return res.ravel()\n",
    "    \n",
    "print( fTrue )\n",
    "refSoln = vec( L_CVX, S_CVX)\n",
    "def errFcnL( Lvec ):\n",
    "    L   = Lvec.reshape( (m,n) )\n",
    "    S   = S_from_L(L)\n",
    "    return norm( vec(L,S)-refSoln )/norm(refSoln)\n",
    "\n",
    "LSvec0 = np.zeros(2*m*n)  # starting point\n",
    "Lvec, data = gradientDescent( f, grad, np.zeros(m*n), prox, prox_obj, stepsize = 0.5, tol=1e-16, maxIters=1e4,\n",
    "    errorFunction=errFcnL, ArmijoLinesearch=False, acceleration=True, restart=250, saveHistory=True )\n",
    "    # TODO: if restart < 0, set saveHistory = True\n",
    "    # NOTE: I fiddled with fNew, make sure that still passes unit tests. Should do better?\n",
    "L   = Lvec.reshape( (m,n) )\n",
    "S   = S_from_L(L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.  Error\n",
      "-----  -------\n",
      "    0  2.69e-01\n",
      " 1500  1.08e-01\n",
      " 3000  4.76e-02\n",
      " 4500  2.62e-02\n",
      " 6000  1.45e-02\n",
      " 7500  7.96e-03\n",
      " 9000  4.33e-03\n",
      "10500  2.35e-03\n",
      "12000  1.27e-03\n",
      "13500  6.86e-04\n",
      "15000  3.70e-04\n",
      "16500  2.00e-04\n",
      "18000  1.08e-04\n",
      "19500  5.81e-05\n",
      "21000  3.13e-05\n",
      "22500  1.69e-05\n",
      "24000  9.07e-06\n",
      "25500  4.84e-06\n",
      "27000  2.56e-06\n",
      "28500  1.33e-06\n"
     ]
    }
   ],
   "source": [
    "L   = Y\n",
    "maxIters = int(3e4)\n",
    "printEvery = int( maxIters/20 )\n",
    "print(\"Iter.  Error\")\n",
    "print(\"-----  -------\")\n",
    "for k in range(maxIters):\n",
    "    S   = prox_l1( Y-L, 1, lambdaS )\n",
    "    L   = prox_nuclearNorm( Y-S, 1, lambdaL )\n",
    "    err = errFcn( vec(L,S) )\n",
    "    if not k % printEvery :\n",
    "        print(f\"{k:5d}  {err:.2e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Douglas-Rachford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.  Objective Error\n",
      "-----  --------- -------\n",
      "    0  1.05e-01  6.79e-01\n",
      "   50  7.20e-02  2.88e-01\n",
      "  100  7.01e-02  1.51e-01\n",
      "  150  6.98e-02  8.47e-02\n",
      "  200  6.97e-02  4.31e-02\n",
      "  250  6.97e-02  2.06e-02\n",
      "  300  6.97e-02  9.88e-03\n",
      "  350  6.97e-02  4.73e-03\n",
      "  400  6.97e-02  2.26e-03\n",
      "  450  6.97e-02  1.09e-03\n",
      "  500  6.97e-02  5.22e-04\n",
      "  550  6.97e-02  2.51e-04\n",
      "  600  6.97e-02  1.21e-04\n",
      "  650  6.97e-02  5.82e-05\n",
      "  700  6.97e-02  2.80e-05\n",
      "  750  6.97e-02  1.35e-05\n",
      "  800  6.97e-02  6.41e-06\n",
      "  850  6.97e-02  3.00e-06\n",
      "  900  6.97e-02  1.36e-06\n",
      "  936  6.97e-02  7.39e-07\n",
      "==  Quitting due to stagnating objective value  ==\n"
     ]
    }
   ],
   "source": [
    "def prox1( LSvec, t ):\n",
    "    L,S = mat(LSvec)\n",
    "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
    "    S   = prox_l1( S, t, lambdaS)\n",
    "    return vec( L, S )\n",
    "\n",
    "def prox2( LSvec, t ):\n",
    "    L,S = mat(LSvec)\n",
    "    L_new   = -t*(S - (1+t)/t*L - Y)/(1+2*t)\n",
    "    S_new   = -t*(L - (1+t)/t*S - Y)/(1+2*t)\n",
    "    return vec( L_new, S_new )\n",
    "\n",
    "LSvec0 = np.zeros(2*m*n)\n",
    "LS, data = DouglasRachford( prox1, prox2, LSvec0, gamma=4e1, F=objective_vec, overrelax = 1.999, \n",
    "    tol =1e-10, maxIters = 1e3, errorFunction=errFcn, printEvery=None )\n",
    "\n",
    "L,S = mat(LS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal-dual method\n",
    "(Chambolle-Pick / Vu / Condat)\n",
    "\n",
    "TBD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burer-Monteiro splitting on $\\|L\\|_*$ term\n",
    "\n",
    "TBD\n",
    "\n",
    "Exploit the fact that $\\|L\\|_* = \\min_{U,V}\\; \\frac12(\\|U\\|_F^2 + \\|V\\|_F^2) \\quad\\text{s.t.}UV^T = L$ and make a change of variables\n",
    "\n",
    "cf. [Adapting Regularized Low Rank Models for Parallel Architectures](https://doi.org/10.1137/17M1147342) Derek Driggs, Stephen Becker, Aleksandr Aravkin (SISC, 2019)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
