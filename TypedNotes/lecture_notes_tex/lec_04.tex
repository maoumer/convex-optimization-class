\documentclass[class=article,crop=false]{standalone} 
\input{../preamble.tex}

\begin{document}

Recall that if a set of points are \emph{linearly independent}, then their linear combinations can equal zero only if all coefficients are zero. Moreover, in an $ n$-dimensional vector space, there can at most be $ n$ linearly independent points.

\begin{defn}[affinely independent]
A set of points $ \{x_i\}_{i=0}^n $ is \allbold{affinely independent} if
\[
	\sum_{ i= 1}^{ n} t_i (x_i -x_0) =0 \implies t_i =0
.\] 
\end{defn}
\begin{remark}
It doesn't matter which $ x_i$ we choose to be $ x_0$. They are all equivalent. This is because an affine space can be think of as a translation of a vector space. That is, every element in the affine space is an element from a vector space offset by the same translation vector. When we subtract any two elements from the affine space, the translation vector cancels out and leaves us an element from the vector space so we go back to linear independence in the vector space. Again we can think of the solutions to inhomogeneous differential equations for a concrete example.
\end{remark}

\begin{remark}
	In a $ n$-dimensional vector space, at most $ n+1$ points can be affinely independent. The "+1" comes from bringing the 0 in the vector space up to $ x_0$, elevating the $ n$-dimensional vector space to an $ n$-dimensional embedding in a $ n+1$-dimensional vector space. For example, $ \rr^{n}$ requires at least $ n$ points to fully describe it via the span. We can visualize $ \rr^{n}$ as an origin-containing plane embedded in $ \rr^{n+1}$. If we translate $ \rr^{n}$ by a vector $ x_0$, we get an $ n$-dimensional affine space that now requires at least $ n+1$ points to describe.
\end{remark}

\begin{defn}[simplex]
For any set of $ k+1$ affinely independent points $ \{x_i\}_{i=0}^n $ in $ \rr^{n}$, they determine a \allbold{simplex}
\[
	C = \conv \left( \{x_i\}_{i=0}^k \right) = \left\{ x= \sum_{ i= 0}^{ k} t_i x_i: t_i \geq 0, \sum_{ i= 0}^{ k} t_i = 1 \right\}  
.\] 
\end{defn}
\begin{note}
The affine dimension of $ k+1$ point simplex is  $ k$, so we call it a  $ k$-dim simplex.
\end{note}
\begin{intuition}
	Due to affine independence, we can think of the convex hull of the points as having "all its fat trimmed". Using the rubber band visualization, we can see why the following examples are true.
\end{intuition}

\begin{eg}
	~\begin{itemize}
		\item $ 1$-dim simplex $ = \conv( \{x_0,x_1\} )$ is a line segment in $ \rr^{n}, n\geq 1$.
		\item $ 2$-dim simplex  $ = \conv( \{x_0,x_1,x_2\} )$ is a filled triangle in $ \rr^{n}, n\geq 2$.
		\item $ 3$-dim simplex is a tetrahedron in $ \rr^{n}, n\geq 3$. Clearly if $ n=2$, 4 points cannot be affinely independent and thus cannot generate a simplex.
		\item In $ \rr^{n}$, the \allbold{unit simplex} is the simplex generated by $ \{0, e_1, e_2, \ldots, e_n\} $, where $ e_i$ is the standard basis. It has affine dimension of $ n$. This can be expressed as
			\[
				\left\{x \in \rr^{n} : x \geq 0, \sum_{ i= 1}^{ n} x_i \leq 1\right\} \text{ or } \{x \in \rr^{n}: x \geq 0, \mathbbm{1}^{T} x \leq 1\} 
			.\]
		\item In $ \rr^{n}$, the $ (n-1)$-dim  \allbold{probability simplex} is generated by $ \{e_1, e_2,\ldots,e_n\} $ (basically unit simplex without 0). It can be expressed as
		\[
			\{x \in \rr^{n}: x\geq 0, \mathbbm{1}^{T} x = 1\} 	
		.\] 
	\end{itemize}
\end{eg}
~\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{./figures/unit_simplex.png}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{./figures/prob_simplex.png}
\end{figure}

\begin{remark}
	Simplex is generated by finite points. \emph{Atomic norm} generalizes this to infinite points and uses gauge functions for signal processing.
\end{remark}

\newpage
\section{Operations that preserve convexity}
\begin{enumerate}[label=\arabic*)]
	\item Cartesian products: If $ C_1 \subseteq \rr^{n_1}$ and $ C_2 \subseteq \rr^{n_2}$ both convex, then $ C_1 \times C_2 \subseteq \rr^{n_1 + n_2}$ is convex.
	\item Arbitrary intersections (even uncountable): If $ C_1, C_2$ are convex, then $ C_1 \cap C_2$ is convex. \emph{This is not true for unions}.
	\item Image and preimage of an affine function $ f(x) = Ax+b$: 
		 \begin{itemize}
			 \item $ f(C)$ is convex if  $ C \subseteq \rr^{n}$ is.
			 \item  $ f^{-1}(C)$ is convex if $ C \subseteq \rr^{m}$ is.
		\end{itemize}
		This implies that scaling, translation, rotation, and projection all preserve convexity.

		So does \allbold{Minkowski sum}:
		\[
		C_1+C_2 \coloneqq \{x+y : x \in C_1, y \in C_2\} 
		.\] 
		We should think of it like a convolution, where each element in $ C_1$ is convolved with the entire $ C_2$.
\end{enumerate}

\subsection{Linear-fractional and perspective functions}
~\begin{defn}[perspective function]
A \allbold{perspective function} is a function $ P: \rr^{n+1} = \rr^{n}\times R_{++} \to \rr^{n}$ s.t. for $ z \in \rr^{n}$ and $ t \in \rr_{++}$,
\[
	P(z, t) = \frac{z}{t}
.\] 
\end{defn}
\begin{intuition}
	We can think of it as normalizing by $ t$, $ (\frac{z}{t},1)$, and then projecting to  $ \rr^{n}$ (or equivalently dropping the last component). Geometrically we can think of it as a "pin-hole" camera that projects 3D points in the $ t$-positive half of $ \rr^{3}$ through the pin-hole at origin onto the 2D film at $ t=-1$. This gives us $ (-\frac{z}{t},-1)$ which is the negative perspective. Then since all the points are on $ t=-1$ we simply drop it.
\end{intuition}
~\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{./figures/perspective.png}
\end{figure}

If $ P$ is the perspective function, we can conclude that
~\begin{itemize}
	\item If $ C \in \rr^{n+1}$ is convex, then $ P(C)$ is convex in $ \rr^{n}$.
	\item If $ C = \rr^{n}$ is convex, then $ P^{-1}(C)$ is convex in $ \rr^{n+1}$.
\end{itemize}

\begin{defn}[linear-fractional function]
A \allbold{linear-fractional function} is $ f: \rr^{n} \to \rr^{m}$ that composes the perspective $ P$ with an affine function $ g$, where
 \[
	 g(x) = \begin{pmatrix} A\\c^{T}\\ \end{pmatrix} x + \begin{pmatrix} b\\d \end{pmatrix} : \rr^{n} \to \rr^{m+1}, A \in \rr^{m \times n}, b \in \rr^{m}, c \in \rr^{n}, d \in \rr
.\] 
That is, $ f=P \circ g$, and
 \[
	 f(x) = \frac{Ax+b}{c^{T}x + d }, \text{ with domain }  \{x: c ^{T} x + d >0\}  
.\] 
\end{defn}

\begin{note}
Since the image and preimage of both affine and perspective functions preserve convexity, the image and preimage of a linear-fractional function again preserve convexity.
\end{note}

\subsection{Generalized inequalities}

\begin{defn}[proper cone]
A cone $ K \subseteq \rr^{n}$ is called a \allbold{proper cone} if it satisfies the following: 
\begin{enumerate}[label=\arabic*)]
	\item convex
	\item closed
	\item solid or nonempty interior
	\item pointed or contains no line or $ x \in K, -x \in K \implies x=0$.
\end{enumerate}
\end{defn}

\begin{prop}
Any proper cone $ K$ induces a partial order:

$x \preceq_{K} y$ or simply $ x\leq y$ if  $ y-x \in K$,

$ x \prec_{K} y$ or simply $ x<y$ if  $ y-x \in \inte(K)$.
\end{prop}

\begin{defn}[dual cone]
If $ K$ is a set, its  \allbold{dual cone} is
\[
K^* = \{y: \langle x,y \rangle \geq 0 \ \forall \ x \in K\} 
.\] 
\end{defn}
\begin{note}
The larger $ K$ is, the more restricted  $ K^* $ becomes.
\end{note}
\subsubsection{Properties of dual cones}
\begin{enumerate}[label=\arabic*)]
	\item $ K^* $ is a cone even if $ K$ isn't a cone.
	\item  $ K^* $ is convex even if $ K$ isn't convex.
	\item  $ K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^* $.
	\item $ K^{* *} =K$ iff $ K$ is a proper cone.
\end{enumerate}

\begin{eg}
If $ K$ is a subspace, then $ K^*  = K^\perp $. This is because $ -x \in K$ so equality is achieved in the definition.
\end{eg}

\begin{eg}[self-dual]
	$ \rr_+^{n} = \left( \rr_{+}^{n} \right)^*  $ because it is a proper cone.
\end{eg}

\begin{eg}[PSD matrices]
	$ K = S_{+}^{n}$, and $ x \in K \implies X = GG^{T}$ (Cholesky). Then
\[
K^*  = \{Y \in S^{n}: \langle Y,X \rangle \geq 0 \ \forall \ X \succeq 0\} 
.\] 
Recall that
\begin{align*}
	\langle Y,X \rangle &= \tr(Y^{T} X) \\
			    &= \tr(YX) \text{ since } Y=Y^{T} \\
			    &= \tr(YGG^{T}) \\
			    &= \tr(G^{T}YG) \text{ by cyclic property of trace} 
\end{align*}
The last expression is $ \geq 0$ for all matrices  $ G$ iff  $ Y \succeq 0$. Hence we show that  $ S_{+}^{n}$ is self-dual.
\end{eg}

\end{document}
