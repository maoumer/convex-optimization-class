\documentclass[class=article,crop=false]{standalone} 
\input{../preamble.tex}

\begin{document}
\begin{eg}
\begin{align*}
	(P) \qquad \qquad \min\quad & \frac{1}{2} \norm{ x-x_0}^2  \\
\text{subject to } \quad &\norm{ Ax-b}\leq \epsilon  
\end{align*}
This is hard to solve directly using gradient descent, as we would need to project which requires finding the SVD of A.
Let $ f(x) = \frac{1}{2} \norm{ x-x_0}^2 $ and $ g(y) = I_{\norm{ y-b}\leq \epsilon }$. Then
\begin{align*}
	f^* (v) = \sup_x \langle v,x \rangle-\frac{1}{2}\norm{ x-x_0} ^2\\
	&= \langle v,x_0 \rangle+ \sup_{\widetilde{ x}} \langle v,\widetilde{ x} \rangle -\frac{1}{2} \norm{ \widetilde{ x}}^2  \\
	&= \langle v,x_0 \rangle + \frac{1}{2} \norm{ v}^2  \\
	g^* (v) &= \sup_{\norm{ y-b}\leq \epsilon } \langle v,y \rangle \\
	&= \langle v,b \rangle+ \sup_{ \norm{ \widetilde{ y}}\leq \epsilon } \langle v,\widetilde{ y} \rangle \\
	&= \langle v,b \rangle + \epsilon \norm{ v}_2  
\end{align*}
Hence we obtain the dual:
\[
	(D) \qquad \min_v \left( \langle A^* v,x_0 \rangle + \frac{1}{2}  \right) 
.\] 
\end{eg}

\subsection{Optimality conditions}
\subsubsection{Complementary slackness}
Suppose we have primal and dual optimal solutions, $ x^* ,\lambda^* ,\nu^* $, and have strong duality (no need for convexity). That is, we assume saddle points exist. Observe
\begin{align*}
	p^* =\qquad \min\quad &f_0(x) = f_0(x^* ) = g(\lambda^* ,\nu^* ) = \inf_x \mathscr{L}(x,\lambda^* ,\nu^* )\\
\text{subject to } \quad &f_i(x) \leq 0, i = 1,\ldots,m \\
&h_i(x) = 0 , i = 1,\ldots,p
\end{align*}
Thus,
\begin{align*}
	p^* &\leq \mathscr{L}(x^* ,\lambda^* ,\nu^* )\\
	    &= f_0(x^* )+ \sum \underbrace{ \lambda_i^*}_{\geq 0} \underbrace{f_i(x^* )}_{\leq 0} + \underbrace{ \sum \nu_i^* h_i(x^* )}_{=0} \\
	    &\leq f_0 (x^* ) = p^*   
\end{align*}
Hence, for all $ i$,  $ \lambda_i^* f_i(x^* ) = 0$. This is called \allbold{complementary slackness}. That is, either $ \lambda_i^* =0$ or $ f_i(x^* ) = 0$.

If $ f_i(x^* )<0$, then $ \lambda_i^* $ must be zero, meaning that the constraints have no effect on the solution (not tight).

\subsubsection{KKT conditions}
~\begin{thm}[KKT 1 "necessary"]
	Suppose $ f_i,h_i$ are differentiable. If $ x^* $ is primal optimal, and $ \lambda^*,\nu^*  $ dual optimal, and no duality gap (strong duality), then necessarily $ x^* ,\lambda^* ,\nu^* $ satisfy
	\begin{enumerate}[label=(\arabic*)]
		\item stationarity: 
			 \[
				 0 = \nabla f_0(x^* ) + \sum \lambda_i^* f_i(x^* ) + \sum \nu_i h_i(x^* ) = \nabla _x \mathscr{L}(x^* ,\lambda^* ,\nu^* )
			.\] 
		\item primal feasibility: $ f_i(x^* )\leq 0, h_i(x^* ) =0$.
		\item dual feasibility: $ \lambda^* \geq 0$.
		\item complementary slackness: $ \lambda_i^* f_i(x^* ) =0$.
	\end{enumerate}
\end{thm}
\begin{note}
There is no need for convexity here.
\end{note}
\begin{remark}
~\begin{enumerate}[label=(\alph*)]
	\item These are only necessary for saddle points and may not be enough without saddle points or strong duality.
		\begin{eg}
		$ \min_x e^{-x}$ satisfies the conditions but has no saddle points. It in fact doesn't have a minimizer. 
		\end{eg}
	\item only needed if differentiable.
	\item If functions are nonconvex, these may not be sufficient. That is, there may be non-optimal solutions.
\end{enumerate}
\end{remark}
\begin{remark}
If the primal problem is convex, then the existence of saddle points guarantees strong duality.
\end{remark}

We can generalize stationarity:
\begin{enumerate}[label=(\arabic*)]
	\item $ x^*  \in \argmin_x \mathscr{L}(x,\lambda^* ,\nu^* )$.

			By convexity, we have
			\[
				0 \in \partial \mathscr{L}(x^* ,\lambda^* ,\nu^* )
			.\] 
			If we have CQ,
			\[
				0 \in \partial f_0(x^* ) + \sum \lambda_i^* \partial f_i(x^* ) + \sum \partial h_i(x^* )
			.\] 
\end{enumerate}

\begin{thm}[KKT 2 sufficient]
	Suppose $ (P)$ is convex ($ f_i$ are convex, $ h_i$ are affine). If $ (x^* ,\lambda^* ,\nu^* )$ solve the KKT conditions, then they are the primal and dual optimal and there is no duality gap.
\end{thm}
\begin{proof}
	Assume $ (x^* ,\lambda^* ,\nu^* )$ satisfies the KKT conditions.
	\begin{align*}
		p^* &\leq f_0(x^* ) \qquad  x^* \text{is feasible }\\
		    &= \mathscr{L}(x^* ,\lambda^* ,\nu^* ) \qquad \text{ since feasible and comp. slack.} \\
		    &\inf_x \mathscr{L}(x,\lambda^* ,\nu^* ) \qquad \text{ stationarity}  \\
		    &= g(alm^* ,\nu^* ) \qquad  \text{ dual feasibility} \\
		    &\leq d^*  
	\end{align*}
	Thus we prove strong duality, and $ p^* =f_0(x^* ) \implies x^* $ is optimal. Same for duals. 
\end{proof}
\end{document}
