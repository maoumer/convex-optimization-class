\documentclass[class=article,crop=false]{standalone} 
\input{../preamble.tex}

\begin{document}
\subsubsection{Recovering a primal solution from a dual solution}
\begin{thm}[BC17 19.1]
	Let $ f \in \Gamma_0(\rr^{n}), g \in \Gamma_0(\rr^{m}), \dom(g) \cap A(\dom f) \neq \O$. The following are equivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item There is no duality gap, and $ x,v$ are primal-dual optimal.  \emph{i.e.} there exists saddle points to $ \mathscr{L}(x,v)=f(x) + \langle Ax,v \rangle - g^* (v)$.
		\item $ A^* v \in \partial f(x)$ and $ -v \in \partial g(Ax)$.
		\item $ x \in \partial f^* (A^* v)$ and $ Ax \in \partial g^* (-v)$.
	\end{enumerate}
\end{thm}
Note that $ 2 \iff 3$ since $ \partial f^* = \partial f^{-1}$ when $ f \in \Gamma_0$.
\begin{prop}[BC17 19.4]
	Under above conditions, if $ f^* $ is differentiable at $ (A^* v)$ (\emph{i.e.} if $ f$ is strictly convex), then either
	\begin{enumerate}[label=(\alph*)]
		\item there is no primal optimal solution, or
		\item $ x = \nabla f^* (A^* v)$ is primal optimal.
	\end{enumerate}
\end{prop}

Let's see a simple example applying Fenchel-Rockafellar duality:
\begin{eg}
\begin{align*}
	(P) \qquad \qquad \min\quad & \frac{1}{2} \norm{ x-x_0}^2  \\
\text{subject to } \quad &\norm{ Ax-b}\leq \epsilon  
\end{align*}
This is hard to solve directly using gradient descent, as we would need to project which requires finding the SVD of A.
Let $ f(x) = \frac{1}{2} \norm{ x-x_0}^2 $ and $ g(y) = I_{\norm{ y-b}\leq \epsilon }$. Then
\begin{align*}
	f^* (v) &= \sup_x \langle v,x \rangle-\frac{1}{2}\norm{ x-x_0} ^2\\
	&= \langle v,x_0 \rangle+ \sup_{\widetilde{ x}} \langle v,\widetilde{ x} \rangle -\frac{1}{2} \norm{ \widetilde{ x}}^2  \\
	&= \langle v,x_0 \rangle + \frac{1}{2} \norm{ v}^2  \\
	g^* (v) &= \sup_{\norm{ y-b}\leq \epsilon } \langle v,y \rangle \\
	&= \langle v,b \rangle+ \sup_{ \norm{ \widetilde{ y}}\leq \epsilon } \langle v,\widetilde{ y} \rangle \\
	&= \langle v,b \rangle + \epsilon \norm{ v}_2  
\end{align*}
Hence we obtain the dual:
\[
	(D) \qquad \min_v \left( \underbrace{ \langle A^* v,x_0 \rangle + \frac{1}{2} \norm{ A^* v}_2^2 }_{f^* (A^* v) } \right) + \left( \underbrace{ \langle v,-b \rangle +  \epsilon \norm{ v}_2 }_{ g^* (-v)} \right)   
,\]
where the first three terms should be differentiable and the last term should have easy-to-find proximity operator.
\end{eg}

\subsection{Optimality conditions}
\subsubsection{Complementary slackness}
Suppose we have primal and dual optimal solutions, $ x^* ,\lambda^* ,\nu^* $, and have strong duality (no need for convexity). That is, we assume saddle points exist. Observe
\begin{align*}
	p^* =\qquad \min\quad &f_0(x) = f_0(x^* ) = g(\lambda^* ,\nu^* ) = \inf_x \mathscr{L}(x,\lambda^* ,\nu^* )\\
\text{subject to } \quad &f_i(x) \leq 0, i = 1,\ldots,m \\
&h_i(x) = 0 , i = 1,\ldots,p
\end{align*}
Thus,
\begin{align*}
	p^* &\leq \mathscr{L}(x^* ,\lambda^* ,\nu^* )\\
	    &= f_0(x^* )+ \sum \underbrace{ \lambda_i^*}_{\geq 0} \underbrace{f_i(x^* )}_{\leq 0} + \underbrace{ \sum \nu_i^* h_i(x^* )}_{=0} \\
	    &\leq f_0 (x^* ) = p^*   
\end{align*}
Hence, for all $ i$,  $ \lambda_i^* f_i(x^* ) = 0$. This is called \allbold{complementary slackness}. That is, either $ \lambda_i^* =0$ or $ f_i(x^* ) = 0$.

If $ f_i(x^* )<0$, then $ \lambda_i^* $ must be zero, meaning that the constraints have no effect on the solution (not tight).

\subsubsection{KKT conditions}
~\begin{thm}[KKT 1 "necessary"]
	Suppose $ f_i,h_i$ are differentiable. If $ x^* $ is primal optimal, and $ \lambda^*,\nu^*  $ dual optimal, and no duality gap (strong duality), then necessarily $ x^* ,\lambda^* ,\nu^* $ satisfy
	\begin{enumerate}[label=(\arabic*)]
		\item stationarity: 
			 \[
				 0 = \nabla f_0(x^* ) + \sum \lambda_i^* \nabla f_i(x^* ) + \sum \nu_i  \nabla h_i(x^* ) = \nabla _x \mathscr{L}(x^* ,\lambda^* ,\nu^* )
			.\] 
		\item primal feasibility: $ f_i(x^* )\leq 0, h_i(x^* ) =0$.
		\item dual feasibility: $ \lambda^* \geq 0$.
		\item complementary slackness: $ \lambda_i^* f_i(x^* ) =0$.
	\end{enumerate}
\end{thm}
\begin{note}
There is no need for convexity here.
\end{note}
\begin{remark}
~\begin{enumerate}[label=(\alph*)]
	\item These are only necessary for saddle points and may not be enough without saddle points or strong duality.
		\begin{eg}
		$ \min_x e^{-x}$ satisfies the conditions but has no saddle points. It in fact doesn't have a minimizer. 
		\end{eg}
	\item only needed if differentiable.
	\item If functions are nonconvex, these may not be sufficient. That is, there may be non-optimal solutions.
\end{enumerate}
\end{remark}
\begin{remark}
If the primal problem is convex, then the existence of saddle points guarantees strong duality.
\end{remark}

We can generalize stationarity:
\begin{enumerate}[label=(\arabic*)]
	\item $ x^*  \in \argmin_x \mathscr{L}(x,\lambda^* ,\nu^* )$.

			By convexity, we have
			\[
				0 \in \partial \mathscr{L}(x^* ,\lambda^* ,\nu^* )
			.\] 
			If we have CQ,
			\[
				0 \in \partial f_0(x^* ) + \sum \lambda_i^* \partial f_i(x^* ) + \sum \nu_i^* \partial h_i(x^* )
			.\] 
\end{enumerate}

\begin{thm}[KKT 2 sufficient]
	Suppose $ (P)$ is convex ($ f_i$ are convex, $ h_i$ are affine). If $ (x^* ,\lambda^* ,\nu^* )$ solve the KKT conditions, then they are the primal and dual optimal and there is no duality gap.
\end{thm}
\begin{proof}
	Assume $ (x^* ,\lambda^* ,\nu^* )$ satisfies the KKT conditions.
	\begin{align*}
		p^* &\leq f_0(x^* ) &&  x^* \text{ is feasible }\\
		    &= \mathscr{L}(x^* ,\lambda^* ,\nu^* ) && \text{ since feasible and comp. slack.} \\
		    &\inf_x \mathscr{L}(x,\lambda^* ,\nu^* ) && \text{ stationarity}  \\
		    &= g(\lambda^* ,\nu^* ) &&  \text{ dual feasibility} \\
		    &\leq d^*  
	\end{align*}
	Thus we prove strong duality, and $ p^* =f_0(x^* ) \implies x^* $ is optimal. Same for duals. 
\end{proof}
\begin{thm}[KKT 3 necessary and sufficient]
	If $ (P)$ is convex and Slater's conditions hold, then  $ x^* $ is primal optional if and only if $ x^* $ with some $ \lambda^*, \nu^* $ satisfy the KKT conditions.
\end{thm}
\begin{proof}
	Slater's implies strong duality and existence of $ \lambda^*, \nu^*$. Hence, either $ x^* $ doesn't exist (\emph{e.g.} min $ e^{-x}$) or it exists and KKT theorems 1 and 2 apply.
\end{proof}
\end{document}
